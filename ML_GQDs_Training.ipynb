{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hazem-Abdelsalam/ML-QDs-electronic-properties/blob/main/ML_GQDs_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install, Import, Upload & Setup"
      ],
      "metadata": {
        "id": "KwS1F5iVlwMl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPNKUiKDls97"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install, Import, Upload & Setup\n",
        "import os\n",
        "\n",
        "# Install chemprop from GitHub if in Colab\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "    try:\n",
        "        import chemprop\n",
        "    except ImportError:\n",
        "        !git clone https://github.com/chemprop/chemprop.git && cd chemprop && pip install . && cd ..\n",
        "        !pip install rdkit-pypi --pre deepchem\n",
        "\n",
        "# Import packages\n",
        "from pathlib import Path\n",
        "from lightning import pytorch as pl\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import re\n",
        "from typing import List\n",
        "from chemprop import data, featurizers, models, nn\n",
        "\n",
        "# Set seed for reproducibility\n",
        "pl.seed_everything(24, workers=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Upload and process CSV"
      ],
      "metadata": {
        "id": "XaQUsaKAnaoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "def upload_and_combine_csvs(\n",
        "    expected_base_names: List[str] = ['GQDs_SiCQDs_Data'],\n",
        "    smiles_columns: List[str] = ['SMILES'],\n",
        "    target_columns: List[List[str]] = [[ 'HOMO', 'LUMO']],\n",
        "    name_column: str = 'Name',  # Column in the CSV containing unique molecule identifiers\n",
        "    output_filename: str = 'combined_cleaned_results.csv'\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    print(\"Please upload your CSV file...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    def find_matching_file(base_name):\n",
        "        # Matches: base_name.csv, base_name (1).csv, etc., case-insensitive\n",
        "        pattern = re.compile(fr'{re.escape(base_name)}(?:\\s*\\(\\d+\\))?\\.csv$', re.IGNORECASE)\n",
        "        for filename in uploaded.keys():\n",
        "            if pattern.fullmatch(filename):\n",
        "                return filename\n",
        "        return None\n",
        "\n",
        "    dfs = []\n",
        "    for base_name, smi_col, tgt_cols in zip(expected_base_names, smiles_columns, target_columns):\n",
        "        matched_file = find_matching_file(base_name)\n",
        "        if not matched_file:\n",
        "            available_files = \"\\n\".join(uploaded.keys())\n",
        "            raise FileNotFoundError(\n",
        "                f\"No file matching '{base_name}' found.\\nAvailable files:\\n{available_files}\"\n",
        "            )\n",
        "\n",
        "        print(f\"\\nâœ… Loading: {matched_file}\")\n",
        "        try:\n",
        "            df = pd.read_csv(matched_file)\n",
        "\n",
        "            # Rename SMILES and target columns to lowercase standard names\n",
        "            df = df.rename(columns={smi_col: 'smiles'})\n",
        "            for tgt_col in tgt_cols:\n",
        "                df = df.rename(columns={tgt_col: tgt_col.lower()})\n",
        "\n",
        "            # Ensure required columns exist\n",
        "            required_cols = ['smiles', name_column] + [col.lower() for col in tgt_cols]\n",
        "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "            if missing_cols:\n",
        "                raise ValueError(f\"Missing columns in {matched_file}: {missing_cols}\")\n",
        "\n",
        "            # Drop rows with missing values and keep only needed columns\n",
        "            df = df[required_cols].dropna().copy()\n",
        "            print(f\"   â†’ {len(df)} valid rows after cleaning\")\n",
        "            dfs.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error processing {matched_file}: {str(e)}\")\n",
        "\n",
        "    if len(dfs) == 0:\n",
        "        raise ValueError(\"No valid data frames to combine.\")\n",
        "\n",
        "    # Combine all data\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "    combined_df.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ Successfully combined data into '{output_filename}'\")\n",
        "    print(f\"ðŸ“Š Total combined rows: {len(combined_df)}\")\n",
        "    print(f\"ðŸ“‹ Columns: {list(combined_df.columns)}\")\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "\n",
        "# === RUN THE FUNCTION ===\n",
        "try:\n",
        "\n",
        "    df_input = upload_and_combine_csvs(\n",
        "        expected_base_names=['cleaned_data_for_retraining'],\n",
        "        smiles_columns=['SMILES'],  # Assuming same column name in both\n",
        "        target_columns=[[ 'HOMO', 'LUMO']],\n",
        "        name_column='Name',  # Specify the \"Name\" column\n",
        "        output_filename='combined_cleaned_results.csv'\n",
        "    )\n",
        "\n",
        "    # Extract data\n",
        "    smis = df_input['smiles'].values\n",
        "    names = df_input['Name'].values  # Extract the \"Name\" column\n",
        "    ys_all = df_input[[ 'homo', 'lumo']].values\n",
        "\n",
        "    # Display summary\n",
        "    print(\"\\nðŸ” Preview of first 5 SMILES, Names, and targets:\")\n",
        "    print(df_input.head())\n",
        "\n",
        "    print(f\"\\nðŸ”¢ Dataset Summary:\")\n",
        "    print(f\"   Total SMILES: {len(smis)}\")\n",
        "    print(f\"   Gap shape: {ys_all.shape}\")\n",
        "    print(f\"   All targets (homo, lumo): {ys_all.shape}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error: {str(e)}\")"
      ],
      "metadata": {
        "id": "gWCj-7Y0nX0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Data Preparation, Training & Prediction"
      ],
      "metadata": {
        "id": "jLS7yRSvl1Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Data Preparation, Training & Prediction\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create MoleculeDatapoints (assuming ys_all is shape [N, 2] for homo, lumo)\n",
        "all_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(smis, ys_all)]\n",
        "mols = [d.mol for d in all_data]\n",
        "\n",
        "# Split: 80% train, 10% val, 10% test\n",
        "train_indices, val_indices, test_indices = data.make_split_indices(mols, \"random\", (0.8, 0.1, 0.1))\n",
        "train_data, val_data, test_data = data.split_data_by_indices(all_data, train_indices, val_indices, test_indices)\n",
        "\n",
        "# Featurizer and Datasets\n",
        "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
        "train_dset = data.MoleculeDataset(train_data[0], featurizer)\n",
        "scaler = train_dset.normalize_targets()\n",
        "\n",
        "val_dset = data.MoleculeDataset(val_data[0], featurizer)\n",
        "val_dset.normalize_targets(scaler)\n",
        "\n",
        "test_dset = data.MoleculeDataset(test_data[0], featurizer)\n",
        "test_dset.normalize_targets(scaler)\n",
        "\n",
        "# DataLoaders\n",
        "num_workers = 4\n",
        "train_loader = data.build_dataloader(train_dset, num_workers=num_workers, shuffle=True)\n",
        "val_loader = data.build_dataloader(val_dset, num_workers=num_workers, shuffle=False)\n",
        "test_loader = data.build_dataloader(test_dset, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Build MPNN Model â€” note: n_tasks=2 for homo and lumo\n",
        "mp = nn.BondMessagePassing()\n",
        "agg = nn.MeanAggregation()\n",
        "output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
        "ffn = nn.RegressionFFN(output_transform=output_transform, n_tasks=2)  # âœ… Changed to 2\n",
        "metric_list = [nn.metrics.RMSE(), nn.metrics.MAE(), nn.metrics.R2Score()]\n",
        "\n",
        "mpnn = models.MPNN(mp, agg, ffn, batch_norm=True, metrics=metric_list)\n",
        "\n",
        "# Trainer with checkpointing\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_top_k=1,\n",
        "    dirpath=\"checkpoints\",\n",
        "    filename=\"best-{epoch}-{val_loss:.2f}\"\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=False,\n",
        "    enable_checkpointing=True,\n",
        "    enable_progress_bar=True,\n",
        "    accelerator=\"auto\",\n",
        "    devices=1,\n",
        "    max_epochs=300,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    deterministic=True\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "trainer.fit(mpnn, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "MOOLvXenl2mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Evaluate, Plot & Save Results"
      ],
      "metadata": {
        "id": "4VvW12zfl9w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Evaluate, Plot & Save Results\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Get predictions\n",
        "test_preds = trainer.predict(mpnn, test_loader)\n",
        "preds = torch.cat(test_preds, dim=0).numpy()\n",
        "true_vals = ys_all[test_indices[0]]  # Shape: (N, 2) for [homo, lumo]\n",
        "test_smiles = [smis[i] for i in test_indices[0]]\n",
        "test_names = [names[i] for i in test_indices[0]]\n",
        "\n",
        "# Create results DataFrame â€” ONLY homo and lumo\n",
        "results_df = pd.DataFrame({\n",
        "    'Name': test_names,\n",
        "    'SMILES': test_smiles,\n",
        "    'True_homo': true_vals[:, 0], 'Pred_homo': preds[:, 0],\n",
        "    'True_lumo': true_vals[:, 1], 'Pred_lumo': preds[:, 1]\n",
        "})\n",
        "\n",
        "# Add error columns for homo and lumo only\n",
        "for tgt in ['homo', 'lumo']:\n",
        "    results_df[f'Error_{tgt}'] = (results_df[f'True_{tgt}'] - results_df[f'Pred_{tgt}']).abs()\n",
        "    results_df[f'Pct_Error_{tgt}'] = (results_df[f'Error_{tgt}'] / (results_df[f'True_{tgt}'] + 1e-8)) * 100\n",
        "    results_df[f'Signed_Error_{tgt}'] = results_df[f'True_{tgt}'] - results_df[f'Pred_{tgt}']\n",
        "\n",
        "# Save to CSV\n",
        "results_df.to_csv('test_set_predictions_with_name.csv', index=False)\n",
        "print(\"âœ… Predictions saved to 'test_set_predictions_with_name.csv'\")\n",
        "\n",
        "# Plotting: True vs Predicted â€” ONLY homo and lumo\n",
        "targets = ['homo', 'lumo']\n",
        "colors = ['green', 'orange']\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "for i, tgt in enumerate(targets):\n",
        "    t = results_df[f'True_{tgt}']\n",
        "    p = results_df[f'Pred_{tgt}']\n",
        "    r2 = r2_score(t, p)\n",
        "    mae = mean_absolute_error(t, p)\n",
        "    rmse = np.sqrt(mean_squared_error(t, p))\n",
        "\n",
        "    axes[i].scatter(t, p, alpha=0.7, edgecolors='w', s=60, color=colors[i])\n",
        "    axes[i].plot([t.min(), t.max()], [t.min(), t.max()], 'r--', label='Ideal')\n",
        "    axes[i].text(0.05, 0.85, f'RÂ² = {r2:.3f}\\nMAE = {mae:.3f}\\nRMSE = {rmse:.3f}',\n",
        "                 transform=axes[i].transAxes, fontsize=12, weight='bold',\n",
        "                 bbox=dict(facecolor='white', alpha=0.8))\n",
        "    axes[i].set_xlabel('True Values (eV)')\n",
        "    axes[i].set_ylabel('Predicted Values (eV)')\n",
        "    axes[i].set_title(f'{tgt.upper()}: True vs Predicted')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final metrics â€” ONLY homo and lumo\n",
        "print(\"\\nðŸ“Š Performance Summary:\")\n",
        "for tgt in ['homo', 'lumo']:\n",
        "    t = results_df[f'True_{tgt}']\n",
        "    p = results_df[f'Pred_{tgt}']\n",
        "    r2 = r2_score(t, p)\n",
        "    mae = mean_absolute_error(t, p)\n",
        "    rmse = np.sqrt(mean_squared_error(t, p))\n",
        "    print(f\"{tgt.upper()}: RÂ²={r2:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}\")\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>> Error Distribution Plots <<<<<<<<<<<<<<<<<<<<\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "for i, (target, color) in enumerate(zip(['homo', 'lumo'], ['green', 'orange'])):\n",
        "    errors = results_df[f'Signed_Error_{target}']\n",
        "    axes[i].hist(errors, bins=15, alpha=0.7, color=color, edgecolor='black')\n",
        "    axes[i].axvline(x=errors.mean(), color='red', linestyle='--', linewidth=2,\n",
        "                    label=f'Mean: {errors.mean():.4f}')\n",
        "    axes[i].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
        "    axes[i].set_xlabel('Prediction Error (eV)')\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "    axes[i].set_title(f'{target.upper()} Error Distribution')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Prediction Error Distributions', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# >>>>>>>>>>>>>>>>>> END <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>> UNIFORM BOLD FONT SETUP <<<<<<<<<<<<<<<<<<<<\n",
        "plt.rcParams.update({\n",
        "    'font.size': 16,\n",
        "    'font.weight': 'bold',\n",
        "    'axes.labelweight': 'bold',\n",
        "    'axes.titleweight': 'bold',\n",
        "    'legend.fontsize': 16,\n",
        "    'xtick.labelsize': 16,\n",
        "    'ytick.labelsize': 16,\n",
        "    'axes.titlesize': 16,\n",
        "    'figure.titlesize': 18\n",
        "})\n",
        "# >>>>>>>>>>>>>>>>>> END FONT SETUP <<<<<<<<<<<<<<<<<<<<<<<<<<<<"
      ],
      "metadata": {
        "id": "zcWeMBd6l_AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of perdicted VS actual Targets"
      ],
      "metadata": {
        "id": "RpO7SKjYT3Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a table for representative quantum dots (EHOMO and ELUMO only)\n",
        "print(\"\\nðŸ“Š Table of True vs Predicted Values for Representative Quantum Dots:\")\n",
        "table_data = []\n",
        "\n",
        "# Select 10 random rows from the results DataFrame\n",
        "representative_samples = results_df.sample(n=10, random_state=42)  # Changed to n=10 as originally intended\n",
        "\n",
        "# Extract relevant data for the table (only homo and lumo)\n",
        "for _, row in representative_samples.iterrows():\n",
        "    name = row['Name']\n",
        "    true_homo, true_lumo = row['True_homo'], row['True_lumo']\n",
        "    pred_homo, pred_lumo = row['Pred_homo'], row['Pred_lumo']\n",
        "    error_homo = abs(true_homo - pred_homo)\n",
        "    error_lumo = abs(true_lumo - pred_lumo)\n",
        "\n",
        "    table_data.append([\n",
        "        name,\n",
        "        true_homo, true_lumo,\n",
        "        pred_homo, pred_lumo,\n",
        "        error_homo, error_lumo\n",
        "    ])\n",
        "\n",
        "# Define columns without any gap-related terms\n",
        "table_columns = [\n",
        "    \"Name\",\n",
        "    \"True EHOMO\", \"True ELUMO\",\n",
        "    \"Pred EHOMO\", \"Pred ELUMO\",\n",
        "    \"Error EHOMO\", \"Error ELUMO\"\n",
        "]\n",
        "\n",
        "table_df = pd.DataFrame(table_data, columns=table_columns)\n",
        "\n",
        "# Display the table\n",
        "print(table_df.to_string(index=False))\n",
        "\n",
        "# Save the table to a CSV file\n",
        "table_df.to_csv(\"representative_predictions_table.csv\", index=False)\n",
        "print(\"\\nâœ… Table saved to 'representative_predictions_table.csv'\")"
      ],
      "metadata": {
        "id": "5vJfj0bYT0yF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}